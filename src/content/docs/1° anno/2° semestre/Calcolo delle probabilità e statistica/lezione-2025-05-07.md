---
title: Lezione (2025-05-07)
---

# Modelli di variabili aleatorie continue (parte 2)

## Gamma

$$
X_{n} \sim Gamma(n, \lambda)
$$

Siano $X_{i} \sim Esp(\lambda),\ i=1,\ldots,n$ variabili aleatorie esponenziali
e sia $X_{n} = \sum_{i =1}^{n} X_{i}$.

Una tale variabile aleatoria può rappresentare il periodo complessivo di
funzionamento di una serie di $n$ elementi utilizzati uno dopo l'altro appena si
verifica il guasto del precedente.

Sia $Y$ il numero di elementi rotti e vogliamo che il periodo di funzionamento
complessivo sia $> x$, ovvero:

$$
\mathbb{P}(X_{n} > x) = 1 - F_{X_{n}}(x) = \sum_{y=0}^{n-1} \frac{(\lambda\ x)^{y}}{y!}\ e^{-\lambda\ x} = \mathbb{P}(Y < n)
$$

- $F_{X_{n}}(x) = \sum_{y=n}^{\infty} \frac{(\lambda\ x)^{y}}{y!}\ e^{-\lambda\ x}$
- $f_{X_{n}}(x) = \frac{dF_{X_{n}}}{dx}(x) = \sum_{y=n}^{\infty} \frac{(\lambda\ x)^{y}}{y!}\ e^{-\lambda\ x} - \sum_{y=n}^{\infty} \frac{\lambda\ (\lambda\ x)^{y}\ e^{-\lambda\ x}}{y!} = \lambda\ e^{-\lambda\ x}\ \frac{(\lambda\ x)^{n-1}}{(n-1)!}$
- $\Phi_{X_{n}}(t) = \left( \frac{\lambda}{\lambda - t} \right)^{n}$,
  $t < \lambda$

Si può definire anche per $n>0$ non intero, allora:

$$
f_{X_{n}}(x) = \frac{\lambda^n}{\Gamma(n)} x^{n-1} e^{-\lambda x} \quad \lambda,n > 0
$$

$$
\Gamma(t) = \begin{dcases}
t \in \mathbb{N} \implies (t-1)! \\
t \notin \mathbb{N} \implies \int_{0}^{\infty} x^{t - 1}\ e^{-x}\ dx
\end{dcases}
$$

In particolare $\Gamma(\frac{1}{2}) = \sqrt{\pi}$

## Normale (aka Gaussiana)

Modello interpretativo di errori o scostamenti del tipo $X - \mu$, dal valore
vero $\mu$.

$$
U = \frac{X - \mu}{\sigma}
$$

- $U$: scostamento normalizzato. Esprime errori di misura come multipli della
  loro ampiezza tipica.
- $\sigma$: errore di sigma. Il suo valore è $\sqrt{\sigma^{2}} = \sqrt{Var[X]}$

Se le misurazioni non sono affette da errori sistematici, allora:

- la media degli errori di misura è nulla;
- la pdf è simmetrica e tende a $0$ per valori che crescono (è molto improbabile
  avere un scostamento dal valore atteso molto grande);
- la pdf ha un unico massimo in corrispondenza di $0$;

Le precedenti proprietà si possono scrivere come se $f_{U}$ fosse pdf della
gaussiana standard, quindi:

$$
\frac{df_{U}}{du}(u) = -f_{U}(u)\ u,\quad f_{U} > 0
$$

Una soluzione di questa equazione differenziale è:

$$
f_{U}(u) = k\ e^{-\frac{u^{2}}{2}}
$$

Perché essa sia densità:

$$
1 = \int_{-\infty}^{+\infty} f_{U}(u)\ du = k\ \int_{-\infty}^{+\infty} e^{- \frac{u^{2}}{2}}\ du = 2\ k\ \int_{0}^{+\infty} e^{- \frac{u^{2}}{2}}\ du = \frac{2}{\sqrt{2}}\ k\ \int_{0}^{+\infty} t^{\frac{1}{2} - 1}\ e^{-t}\ dt = \frac{2}{\sqrt{2}}\ k\ \Gamma(\frac{1}{2})
$$

Quindi $k=\frac{1}{\sqrt{2\pi}}$

### Normale standard

$$
U \sim N(0,1)
$$

è una variabile aleatoria normale con media $0$ e $\sigma=1$.

- $f_{U}(u) = \frac{1}{\sqrt{2\pi}}\ e^{\frac{- u^{2}}{2}}$
- $\Phi_{U}(t) = e^{\frac{t^{2}}{2}}$
- $\mathbb{E}[U] = 0$

### Normale non standard

$$
X \sim N(\mu, \sigma^{2})
$$

$$
X = \mu  + \sigma U
$$

Usando le proprietà della variabile aleatoria, risulta:

$$
f_{X}(x) = \frac{1}{\sigma\ \sqrt{2\pi}}\ e^{- \frac{1}{2}\ (\frac{x - \mu}{\sigma})^{2}}
$$

- $\Phi_{X}(t) = e^{t\ \mu + t^{2} \frac{\sigma^{2}}{2}}$
- $\mathbb{E}[X] = \mathbb{E}[\mu + \sigma\ U] = \mu + \sigma\ \mathbb{E}[U] = \mu$
- $Var[X] = \sigma^{2}$

:::tip Per semplificare la risoluzione di problemi, di solito si trasformano le
normali non standard in normali standard.

:::

### Proprietà

Siano $X_{1}$ e $X_{2}$ gaussiane standard indipendenti. Se $Y = X_{1} + X_{2}$,
allora $\Phi_{Y} = \Phi_{X_{1}}\ \Phi_{X_{2}} = e^{2\ \frac{t^{2}}{2}}$, quindi
$Y$ diventa una v.a. normale non standard.

Siano $X_{i}$ variabili aleatorie indipendenti e gaussiane non standard del tipo
$N(\mu_{i}, \sigma_{i}^{2})$ e sia $Y = \sum_{i=1}^{n} a_{i} X_{i}$, allora:

- $\mathbb{E}[Y] = \sum_{i=1}^{n} a_{i}\ \mu_{i}$
- $Var[Y] = \sum_{i=1}^{n} (a_{i}\ \sigma_{i})^{2}$
- $\Phi_{Y}(t) = \prod_{i=1}^{n} \Phi_{X_{i}}(t) = e^{t \sum_{i} a_{i}\ \mu_{i} + \frac{t^{2} \sum_{i} (a_{i}\ \sigma_{i})^{2}}{2}}$
- $Y \sim N(\sum_{i} a_{i}\ \mu_{i}, \sum_{i} (a_{i}, \sigma_{i})^{2})$

:::info Questo significa che la gaussiana è riproducibile, ovvero che la somma
di variabili gaussiane restituisce una nuova variabile gaussiana. Questo non è
sempre vero per altri modelli di variabili aleatorie.

:::

Considero i coefficienti $a_{1},\ldots,a_{n}$ e $b_{1}, \ldots, b_{n}$ e
$Y = \sum_{i} a_{i}\ X_{i}$ e $Z = \sum_{i} b_{i}\ X_{i}$.

$$
Cov(Y,Z) = \mathbb{E}[(Y - \mathbb{E}[Y]) (Z - \mathbb{E}[Z])] = \sum_{i} a_{i}\ b_{i}\ \sigma_{i}^{2}
$$

Se $\forall\ i,\ \sigma_{i} = \sigma$, allora:

$$
Cov[Y,Z] = 0 \iff \sum_{i} a_{i} b_{i} = 0
$$

In questo caso $Y$ e $Z$ sono dette ortogonali.

### Normale bivariata

Siano $X \sim N(\mu_{1}, \sigma_{1}^{2})$ e $Y \sim N(\mu_{2}, \sigma_{2}^{2})$
variabili aleatorie normali e indipendenti. Per semplicità $\mu_{1} = \mu_{2}$.

Allora possiamo definire:

$$
f(x,y) = f_{X}(x)\ f_{Y}(y) = \frac{1}{2 \pi\ \sigma_{1}\ \sigma_{2}}\ e^{- \frac{1}{2}\ \left(\frac{x^{2}}{\sigma_{1}^{2}} + \frac{y^{2}}{\sigma_{2}^{2}}\right)}
$$

Siano $X_{1}, \ldots, X_{n}$ v.a. normali standard, allora
$f_{X_{1}, \ldots, X_{n}} = \frac{1}{(\sqrt{2\pi})^{n}}\ e^{-\frac{1}{2} (x_{1}^{2} + \ldots + x_{n}^{2})}$
è definita gaussiana standard multivariata.
