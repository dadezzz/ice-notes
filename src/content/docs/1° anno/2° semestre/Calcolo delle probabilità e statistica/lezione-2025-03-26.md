---
title: Lezione (2025-03-26)
---

# Media di una variabile aleatoria

La media (valore atteso) fornisce il 'baricentro' della variabile aleatoria.

Fissata una variabile aleatoria $X$ e la funzione $\phi(X)$, definiamo
l'operatore che fornisce il valore atteso come:

$$
\mathbb{E}[\phi(X)] = \begin{cases}
\displaystyle X \text{ continua} \implies \int_{-\infty}^{+\infty} \phi(x)\ f(x)\ dx \\[5pt]
\displaystyle X \text{ discreta} \implies \sum_{i=-\infty}^{+\infty} \phi(x_{i})\ P(x_{i})
\end{cases}
$$

:::warning Solo dove la serie e l'integrale sono assolutamente convergenti.

:::

## Proprietà

- In generale, $\mathbb{E}$ si può applicare anche a $X^{n}$ e il valore
  $\mathbb{E}[X^{n}]$ è detto **momento** $n$-**esimo centrato in** $X$.
- $\mathbb{E}$ è lineare, quindi $\mathbb{E}[a + b\ X] = a + b\ \mathbb{E}[X]$.

# Media di vettori aleatori

Sia $(X,Y)$ un vettore aleatorio bidimensionale. Definiamo:

$$
\mathbb{E}[\phi(X,Y)] = \begin{dcases}
\displaystyle (X,Y) \text{ discreta} \implies \sum_{i} \sum_{j} \phi(x_{i}, y_{j})\ P(x_{i}, y_{j}) \\[5pt]
\displaystyle (X,Y) \text{ continua} \implies \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} \phi(x, y)\ f(x, y)\ dx\ dy
\end{dcases}
$$

## Proprietà

- $\mathbb{E}[X + Y] = \mathbb{E}[X]\ + \mathbb{E}[Y]$
- Se $X$ e $Y$ sono stocasticamente indipendenti, allora
  $\mathbb{E}[X\ Y] = \mathbb{E}[X]\ \mathbb{E}[Y]$

# Varianza

In pratica la varianza di $X$ è la media tra i valori di scarto rispetto alla
media di $X$.

$$
Var[X] = \mathbb{E}[(X - \mathbb{E}[X])^{2}] = \begin{dcases}
X \text{ discreta} \implies \sum_{x} (x - \mathbb{E}(X))^{2}\ P(x) \\[5pt]
X \text{ continua} \implies \int_{-\infty}^{+\infty} (x - \mathbb{E}(X))^{2}\ f(x)\ dx
\end{dcases}
$$

La varianza è definita solo quando la sommatoria o l'integrale convergono.

## Proprietà

- $Var[X]$ si può calcolare anche come
  $\mathbb{E}[X^{2}] - (\mathbb{E}[X])^{2}$.
- $Var[X]$ non è lineare, per esempio $Var[a + b\ X] = b^{2}\ Var[X]$.
- Se $X$ e $Y$ sono indipendenti, allora $Var[X + Y] =  Var[X]  + Var[Y]$,
  altrimenti è necessario considerare la covarianza:
  $Var[X + Y] = Var[X] + Var[Y] + 2\ Cov[X,Y]$.

# Covarianza

La covarianza misura la tendenza delle 2 variabili ad assumere valori
simultaneamente maggiori o minori delle rispettive medie.

$$
Cov[X,Y]  = \mathbb{E}[X - \mathbb{E}[X]]\ \mathbb{E}[Y - \mathbb{E}[Y]] = \mathbb{E}[XY] - \mathbb{E}[X]\ \mathbb{E}[Y]
$$

Se $X$ e $Y$ sono indipendenti, allora $Cov[X,Y] = 0$.

### Relazione tra varianza e covarianza

$$
Cov[X,Y]^{2} \leq Var[X]\ Var[Y]
$$

Se $X$ e $Y$ sono legate da una relazione lineare, ovvero $Y = a\ X + b$, allora
vale la relazione:

$$
Cov[X,Y]^{2} = Var[X]\ Var[Y]
$$

\
