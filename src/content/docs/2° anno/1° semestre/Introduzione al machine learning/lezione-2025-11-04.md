---
title: Lezione (2025-11-04)
---

## Classificazione di testi

Per classificare un testo in base al suo contenuto devo ottenere un vettore
input.

Prendo un vettore con le $n$ parole più significative del 'dizionario'. Poi per
ogni parola trovata nel testo, aumento di 1 la corrispettiva componente nel
vettore (bag of words).

:::info

Ignoriamo per il momento che viene persa l'informazione data dall'ordine delle
parole.

:::

Otteniamo un grande vettore di ingresso. Ora dobbiamo capire quali sono le
features più significative. Per trovare correlazioni tra coppie di parole posso
calcolare la loro indipendenza.

:::info

$$
|\fP{\text{word}_1, \text{word}_2} - \fP{\text{word}_1} \fP{\text{word}_2}|
$$

Se il valore della differenza è alto, allora c'è alta dipendenza tra le
variabili.

:::

### Test del Chi quadro

Si usa per testare la dipendenza degli output dall'input.

$$
\Chi^2 = \sum_{y,x} \frac{(\text{count}_{y,x} - n \fP{y} \fP{x})^2}{n \fP{y} \fP{x}}
$$

Dove $\text{count}_{y,x}$ è il numero di occorrenze del valore $x$ per una data
classe $y$ e $n$ è il numero totale di esempi nel dataset.

Le features migliori sono quelle con il $\Chi^2$ più grande.

### Entropia

Si usa per misurare quanta informazione fornisce una certa feature, maggiore è
l'incertezza, maggiore sarà il valore:

$$
H(Y) = - \sum_{y \in Y} \fP{y} \fLog{\fP{y}} [bit]
$$

Quando conosco un valore d'ingresso $x_i$, ottengo l'**entropia condizionale**.
Essa sarà minore di quella iniziale.

$$
H(Y | X_i) = \sum_{x_i \in X_i} \fP{x_i} (- \sum_{y \in Y} \fP{y | x_i} \fLog{\fP{y | x_i}}) [bit]
$$

L'**informazione mutua** di $x_i$ è definita come la differenza tra l'entropia
iniziale e quella condizionale.

$$
I(Y, X_i) = H(Y) - H(Y | X_i) = \sum_{y, x_i} \fP{y, x_i} \fLog{\frac{\fP{y, x_i}}{\fP{y} \fP{x_i}}}
$$

Se $y$ e $x_i$ sono indipendenti, allora $I(Y, X_i) = 0$ e possiamo scartare la
feature.

L'informazione mutua è in grado di catturare dipendenze non lineari arbitrarie
tra 2 variabili, quindi ci dice se otteniamo informazioni anche quando il
coefficiente di correlazione è 0.

## Clustering

Il clustering è l'operazione di riconoscere nuovi input (in maniera non
supervisionata) e assegnare etichette ad essi.

A livello matematico, significa riconoscere delle singole varietà all'interno di
spazi altamente dimensionali. Fatto questo, molte features diventeranno
ridondanti e quindi potranno essere scartate.

La compressione dell'informazione avviene attraverso i prototipi. Ogni prototipo
riassume l'informazione contenuta nel sottoinsieme di casi che rappresenta. In
questo caso riassumere significa una funzione $f: \R^n \to \R^m$ dove $n < m$.

### Approci per l'apprendimento non supervisionato

- top-down: decido il numero di classi e poi divido i miei dati in queste
  classi.
- bottom-up: inizio a raggruppare i dati e mi fermo quando ha senso per la
  specifica applicazione.
