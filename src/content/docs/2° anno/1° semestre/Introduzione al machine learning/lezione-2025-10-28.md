---
title: Lezione (2025-10-28)
---

## Varietà

In geometria, una varietà è uno spazio che localmente è simile ad uno spazio
euclideo, ma che globalmente può avere proprietà diverse.

In una varietà è possibile interpolare linearmente 2 punti vicini.

## Feature selection

Nel modo reale, i dati che usiamo per allenare un modello possono essere
duplicati, irrilevanti o mancanti.

Bisogna quindi prendere un sottoinsieme dei dati per ottenere i seguenti
vantaggi:

- riduzione di dimensionalità;
- riduzione dell'utilizzo di memoria;
- miglior generalizzazione;
- maggiori possibilità di spiegare come funziona il modello (dovuto al minor
  numero di dimensioni);

Il processo di feature selection non ha una soluzione unica, serve molto intuito
e conoscenza del dominio. Ci sono però 3 di metodi efficaci:

- **Wrapper methods**: Si testa ogni combinazione di features del training set,
  poi si tiene quella che dà l'errore minore.

  Per disabilitare una feature, moltiplico il vettore di input per uno dove le
  componenti sono 0 (feature disabilitata) o 1 (feature abilitata). Quindi dato
  un input a $n$ dimensioni, avrò $2^n$ possibili combinazioni di features.

- **Filter methods**: Al posto di usare l'errore, che richiede il training di
  molti modelli diversi (costoso). Si possono provare ad usare altri indici che
  predicono quanto una variabile potrebbe influire sul risultato finale. Alcuni
  esempi sono la correlazione l'informazione mutua tra 2 variabili.

- **Embedded methods**: Le features vengono selezionate durante il training,
  portando a $0$ i pesi di quelle che si vuole disabilitare.

Ci sono 2 modi di procedere per scegliere le features:

- bottom-up: si inseriscono features in ordine di importanza fino a quando
  l'errore non smette di scendere;
- top-down: si eliminano progressivamente features fino a quando si ottengono le
  performance migliori (sia di errore, sia di grandezza del modello);

### Metodi di selezione

#### Valore dei pesi

Il metodo più intuitivo per definire l'importanza di una feature potrebbe essere
quello di osservare il valore del suo peso e tenere solo quelli più
significativi.

:::caution

Questo metodo funziona solo se il valore dell'input era stato normalizzato in
precedenza.

:::

Inoltre ci sono features che non hanno nessun valore quando vengono prese da
sole. Però la presenza di altri input può cambiare il risultato (si pensi a
situazioni simili ad un XOR logico).

:::caution

Misurare alcune features in maniera isolata, può rimuovere dal modello le
relazioni mutuali tra le diverse componenti. Quindi i metodi di filtro possono
talvolta darci informazioni non corrette.

:::

#### Coefficiente di correlazione

Per misurare relazioni di tipo lineare (che formano una retta su uno scatter
plot) tra 2 variabili, si usa il coefficiente di correlazione di Pearson:

$$
\rho_{X_i, Y} = \frac{\text{cov}[X_i, Y]}{\sigma_{X_i} \sigma_Y} = \frac{\mathbb{E}[(X_i - \mu_{X_i})(Y - \mu_Y)]}{\sigma_{X_i} \sigma_Y}
$$

Per 2 variabili altamente correlate (valore vicino a $1$ o $-1$), la conoscenza
di $X_i$, ci dice quale sarà il valore di $Y$. Quindi quella feature ha alta
influenza sul valore del risultato e perciò non va scartata.

Anche se una feature ha correlazione $0$, non è detto che essa non contenga
informazioni sull'output, la loro relazione potrebbe essere di tipo non lineare.

![Scatter plot di diversi tipi di dati correlati e non](../../../../../images/introduzione-al-machine-learning/scatter-plot-coefficiente-correlazione.png)
