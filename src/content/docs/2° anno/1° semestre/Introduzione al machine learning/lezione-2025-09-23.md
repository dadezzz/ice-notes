---
title: Lezione (2025-09-23)
---

## Apprendimento supervisionato

Il nostro obiettivo è quello di costruire una funzione $y = f(\mathbf{x})$.

Si definisce **apprendimento supervisionato** perchè ci vuole un supervisore (un
umano o anche una macchina) che crea un database di dati esempio.

Dobbiamo imporre una restrizione, ovvero che **l'argomento $\mathbf{x}$ abbia la
stessa distribuzione di probabilità degli esempi che il sistema ha già visto**.

Una distribuzione di probabilità rappresenta la probabilità che le componenti
dei vettori esempio e input assumano certi valori. La nostra $f$ darà risultati
attendibili solo quando i vettori esempio e quello che viene dato in input
seguono la stessa distribuzione di probabilità.

L'apprendimento supervisionato è solo uno dei modi di arrivare ad una funzione
che preso un input generale riesca a restituire un output.

La nostra funzione (d'ora in poi modello) esprimerà l'associazione tra l'input e
l'output dato attraverso dei parametri che possono essere modificati per
migliorare la correttezza dell'output.

$$
y = f(\mathbf{x}, \mathbf{w})
$$

Il processo di apprendimento sta nel trovare i migliori parametri in maniera
automatica. Ecco perchè si chiama machine learning.

### Nomenclatura

Di seguito definiamo alcuni termini che vengono spesso usati nel campo:

- **Features**: rappresentano una caratteristica misurabile di ciò che viene
  dato in input al modello. Di solito una feature corrisponde ad una componente
  di $\mathbf{x}$.
- **Pesi**: in inglese ci si riferisce alle componenti del vettore $\mathbf{w}$
  come weigths.
- **Ottimizzazione**: processo con il quale si trovano i pesi che permettono di
  abbassare la percentuale di errori commessi dal modello.

In generale l'output di un modello si può definire in 2 modi:

- **Regressione**: il modello, partendo dall'input dato, calcola un numero
  reale.
- **Classificazione**: il modello restituisce sempre un numero, ma questo numero
  appartiene ad un insieme finito di possibili valori.

### Come trovare i pesi

Iniziamo a calcolare $f$ sull'insieme degli esempi. Con parametri casuali il
modello darà risultati che possono essere giusti o sbagliati.

Definiamo una misura (che chiameremo **errore**) di quanto una risposta è
sbagliata rispetto a quella dell'etichetta dell'esempio dato. Il modo per
ottimizzare i parametri è quello di trovare il valore che minimizzi la funzione
di errore.

Una buona misura di errore può essere la somma degli errori fatti dalla funzione
sugli esempi al quadrato (aka la varianza):

$$
E(\mathbf{w}) = \sum_{i} (f(\mathbf{x}_i, \mathbf{w}) - y_i)^2
$$

Se $E(\mathbf{w})$ è una funzione continua, allora posso 'spostarmi' verso il
punto con il valore più basso.

**Gradient descent**: dato che il computer non può trovare il minimo della
funzione in maniera visiva (come farebbe un umano a colpo d'occhio), l'algoritmo
che si usa è quello di spostarsi di piccoli intervalli sulla funzione nella
direzione del gradiente di $E(w)$. In questo modo eventualmente arriverò ad un
punto di minimo dove $\nabla E(w) = 0$.

:::tip

Si elevano gli errori al quadrato e non si usa il valore assoluto perchè
l'elevamento a potenza dà una funzione derivabile, mentre il valore assoluto no.

:::

## Come testare un modello

Nell'apprendimento supervisionato, è importante tenere ben separati l'insieme di
dati esempio usati per l'allenamento e l'insieme di dati usati per testare la
performance del sistema.

Quindi un giudizio finale si può dare in base al numero di risposte corrette
date dal sistema sui dati di test su cui non è stato allenato.

Il metodo più corretto sarebbe quello di dividere in 3 insiemi i dati:

- **training set**: usato per ottimizzare i parametri attraverso il gradient
  descent;
- **validation set**: usato per validare un modello, dà risultati ottimistici
  perchè noi stiamo cambiando il modello per ottenere performance migliori su
  questo set;
- **testing set**: si usa come prova finale per vedere se ci sono discrepanze
  tra il validation set e un caso più generale;

### $K$-fold cross-validation

Tecnica usata per decidere come suddividere i dati esempio negli insiemi di
training e validazione.

1. Divido l'insieme totale in $K$ sottoinsiemi.
2. Ne tengo uno da parte come validation set.
3. Ogni volta che modifico il modello scelgo un'altro sottoinsieme di dati come
   validation set.
4. Faccio una media tra tutti i risultati di validazione.

In questo modo si evita di concentrarsi ad ottimizzare il modello per uno
specifico set di dati di validazione e ci si può aspettare che il risultato del
testing set sia più simile a quello atteso.

Come scegliere un valore ottimale di $K$? La ricerca è ancora in corso.
